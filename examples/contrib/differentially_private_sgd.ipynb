{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJHywE_oL3j2"
   },
   "source": [
    "# Differentially private convolutional neural network on MNIST.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/google-deepmind/optax/blob/main/examples/differentially_private_sgd.ipynb)\n",
    "\n",
    "A large portion of this code is forked from the differentially private SGD\n",
    "example in the [JAX repo](\n",
    "https://github.com/jax-ml/jax/blob/main/examples/differentially_private_sgd.py).\n",
    "\n",
    "To run the colab locally you need install the\n",
    "`dp-accounting`, `tensorflow`, `tensorflow-datasets`, packages via `pip`.\n",
    "\n",
    "\n",
    "[Differentially Private Stochastic Gradient Descent](https://arxiv.org/abs/1607.00133) requires clipping the per-example parameter\n",
    "gradients, which is non-trivial to implement efficiently for convolutional\n",
    "neural networks.  The JAX XLA compiler shines in this setting by optimizing the\n",
    "minibatch-vectorized computation for convolutional architectures. Train time\n",
    "takes a few seconds per epoch on a commodity GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VaYIiCnjL3j3"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import dp_accounting\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from optax import contrib\n",
    "from optax import losses\n",
    "import optax\n",
    "from jax.example_libraries import stax\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Shows on which platform JAX is running.\n",
    "print(\"JAX running on\", jax.devices()[0].platform.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7Dn8L_Uw0Yb"
   },
   "source": [
    "This table contains hyperparameters and the corresponding expected test accuracy.\n",
    "\n",
    "\n",
    "| DPSGD  | LEARNING_RATE | NOISE_MULTIPLIER | L2_NORM_CLIP | BATCH_SIZE | NUM_EPOCHS | DELTA | FINAL TEST ACCURACY |\n",
    "| ------ | ------------- | ---------------- | ------------ | ---------- | ---------- | ----- | ------------------- |\n",
    "| False  | 0.1           | NA               | NA           | 256        | 20         | NA    | ~99%                |\n",
    "| True   | 0.25          | 1.3              | 1.5          | 256        | 15         | 1e-5  | ~95%                |\n",
    "| True   | 0.15          | 1.1              | 1.0          | 256        | 60         | 1e-5  | ~96.6%              |\n",
    "| True   | 0.25          | 0.7              | 1.5          | 256        | 45         | 1e-5  | ~97%                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jve2h810L3j3"
   },
   "outputs": [],
   "source": [
    "# Whether to use DP-SGD or vanilla SGD:\n",
    "DPSGD = True\n",
    "# Learning rate for the optimizer:\n",
    "LEARNING_RATE = 0.25\n",
    "# Noise multiplier for DP-SGD optimizer:\n",
    "NOISE_MULTIPLIER = 1.3\n",
    "# L2 norm clip:\n",
    "L2_NORM_CLIP = 1.5\n",
    "# Number of samples in each batch:\n",
    "BATCH_SIZE = 256\n",
    "# Number of epochs:\n",
    "NUM_EPOCHS = 15\n",
    "# Probability of information leakage:\n",
    "DELTA = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLGeV4y4DBkL"
   },
   "source": [
    "CIFAR10 and CIFAR100 are composed of 32x32 images with 3 channels (RGB). We'll now load the dataset using `tensorflow_datasets` and display a few of the first samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zynvtk4wDBkL"
   },
   "outputs": [],
   "source": [
    "(train_loader, test_loader), info = tfds.load(\n",
    "    \"mnist\", split=[\"train\", \"test\"], as_supervised=True, with_info=True\n",
    ")\n",
    "\n",
    "min_max_rgb = lambda image, label: (tf.cast(image, tf.float32) / 255., label)\n",
    "train_loader = train_loader.map(min_max_rgb)\n",
    "test_loader = test_loader.map(min_max_rgb)\n",
    "\n",
    "train_loader_batched = train_loader.shuffle(\n",
    "    buffer_size=10_000, reshuffle_each_iteration=True\n",
    ").batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "NUM_EXAMPLES = info.splits[\"test\"].num_examples\n",
    "test_batch = next(test_loader.batch(NUM_EXAMPLES, drop_remainder=True).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6In7oQ-0EhG"
   },
   "outputs": [],
   "source": [
    "init_random_params, predict = stax.serial(\n",
    "    stax.Conv(16, (8, 8), padding=\"SAME\", strides=(2, 2)),\n",
    "    stax.Relu,\n",
    "    stax.MaxPool((2, 2), (1, 1)),\n",
    "    stax.Conv(32, (4, 4), padding=\"VALID\", strides=(2, 2)),\n",
    "    stax.Relu,\n",
    "    stax.MaxPool((2, 2), (1, 1)),\n",
    "    stax.Flatten,\n",
    "    stax.Dense(32),\n",
    "    stax.Relu,\n",
    "    stax.Dense(10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2OUgc6J0Jsl"
   },
   "source": [
    "This function computes the privacy parameter epsilon for the given number of steps and probability of information leakage `DELTA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43177TofzuOa"
   },
   "outputs": [],
   "source": [
    "def compute_epsilon(steps):\n",
    "  if NUM_EXAMPLES * DELTA > 1.:\n",
    "    warnings.warn(\"Your delta might be too high.\")\n",
    "  q = BATCH_SIZE / float(NUM_EXAMPLES)\n",
    "  orders = list(jnp.linspace(1.1, 10.9, 99)) + list(range(11, 64))\n",
    "  accountant = dp_accounting.rdp.RdpAccountant(orders)\n",
    "  accountant.compose(dp_accounting.PoissonSampledDpEvent(\n",
    "      q, dp_accounting.GaussianDpEvent(NOISE_MULTIPLIER)), steps)\n",
    "  return accountant.get_epsilon(DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9mPtPvB0D3X"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(params, batch):\n",
    "  images, labels = batch\n",
    "  logits = predict(params, images)\n",
    "  return losses.softmax_cross_entropy_with_integer_labels(logits, labels).mean(), logits\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def test_step(params, batch):\n",
    "  images, labels = batch\n",
    "  logits = predict(params, images)\n",
    "  loss = losses.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "  accuracy = (logits.argmax(1) == labels).mean()\n",
    "  return loss, accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOet-_860ysL"
   },
   "outputs": [],
   "source": [
    "if DPSGD:\n",
    "  tx = contrib.dpsgd(\n",
    "      learning_rate=LEARNING_RATE, l2_norm_clip=L2_NORM_CLIP,\n",
    "      noise_multiplier=NOISE_MULTIPLIER, seed=1337)\n",
    "else:\n",
    "  tx = optax.sgd(learning_rate=LEARNING_RATE)\n",
    "\n",
    "_, params = init_random_params(jax.random.PRNGKey(1337), (-1, 28, 28, 1))\n",
    "opt_state = tx.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-NmP7g01EdA"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(params, opt_state, batch):\n",
    "  grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "  if DPSGD:\n",
    "    # Inserts a dimension in axis 1 to use jax.vmap over the batch.\n",
    "    batch = jax.tree.map(lambda x: x[:, None], batch)\n",
    "    # Uses jax.vmap across the batch to extract per-example gradients.\n",
    "    grad_fn = jax.vmap(grad_fn, in_axes=(None, 0))\n",
    "\n",
    "  grads, _ = grad_fn(params, batch)\n",
    "  updates, new_opt_state = tx.update(grads, opt_state, params)\n",
    "  new_params = optax.apply_updates(params, updates)\n",
    "  return new_params, new_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMl9dnbJ1OtQ"
   },
   "outputs": [],
   "source": [
    "accuracy, loss, epsilon = [], [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  for batch in train_loader_batched.as_numpy_iterator():\n",
    "    params, opt_state = train_step(params, opt_state, batch)\n",
    "\n",
    "  # Evaluates test accuracy.\n",
    "  test_loss, test_acc = test_step(params, test_batch)\n",
    "  accuracy.append(test_acc)\n",
    "  loss.append(test_loss)\n",
    "  print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, test accuracy: {test_acc}\")\n",
    "\n",
    "  #\n",
    "  if DPSGD:\n",
    "    steps = (1 + epoch) * NUM_EXAMPLES // BATCH_SIZE\n",
    "    eps = compute_epsilon(steps)\n",
    "    epsilon.append(eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nsV-9_b2qca"
   },
   "outputs": [],
   "source": [
    "if DPSGD:\n",
    "  _, axs = plt.subplots(ncols=3, figsize=(9, 3))\n",
    "else:\n",
    "  _, axs = plt.subplots(ncols=2, figsize=(6, 3))\n",
    "\n",
    "axs[0].plot(accuracy)\n",
    "axs[0].set_title(\"Test accuracy\")\n",
    "axs[1].plot(loss)\n",
    "axs[1].set_title(\"Test loss\")\n",
    "\n",
    "if DPSGD:\n",
    "  axs[2].plot(epsilon)\n",
    "  axs[2].set_title(\"Epsilon\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ubOEWod3OPj"
   },
   "outputs": [],
   "source": [
    "print(f'Final accuracy: {accuracy[-1]}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
