{
  "cells": [
    {
      "metadata": {
        "id": "-2Bmeh2sl_fp"
      },
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "The purpose of this notebook is to demonstrate example usages of utilities defined by the optax microbatching API.  microbatching is a general purpose function transformation that lifts a function that operates over a batch to one that operates over a potentially much larger batch, by splitting up the work into smaller chunks and accumulating the results.  Like other jax transformations, it's designed to be quite general - any function that can normally be traced by other jax transformations should work here. This notebook is broken up into multiple sections to illustrate usages of different functions in the API."
      ]
    },
    {
      "metadata": {
        "id": "5LUEGvWml-mH"
      },
      "cell_type": "code",
      "source": [
        "# ! pip install -q \"optax @ git+https://github.com/google-deepmind/optax\"\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "from flax import nnx\n",
        "import functools\n",
        "import time\n",
        "from optax import microbatching\n",
        "import gc"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "T5Ie-_tfqgAV"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Here we define a minimal transformer architecture, along with some dummy data to call it with. This notebook is primarily concerned with demonsrating the APIs\n",
        "and looking at throughput numbers (examples processed / second), rather than training a model on real data. This notebook is intended to run on a Google Colab Instance with T4 GPU type.  If you run on different hardware, you may need to scale down the model size via the configuration below."
      ]
    },
    {
      "metadata": {
        "id": "nInGNjjeqcvU"
      },
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nnx.Module):\n",
        "  def __init__(self, hidden_size: int, num_heads: int, *, rngs: nnx.Rngs):\n",
        "    self.norm1 = nnx.LayerNorm(hidden_size, rngs=rngs)\n",
        "    self.mha = nnx.MultiHeadAttention(num_heads, hidden_size, rngs=rngs)\n",
        "    self.norm2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n",
        "    self.mlp = nnx.Sequential(\n",
        "        nnx.Linear(hidden_size, 4*hidden_size, rngs=rngs),\n",
        "        jax.nn.gelu,\n",
        "        nnx.Linear(4*hidden_size, hidden_size, rngs=rngs),\n",
        "    )\n",
        "\n",
        "  def __call__(self, x: jax.Array) -> jax.Array:\n",
        "    attention_output = self.mha(self.norm1(x), self.norm1(x), self.norm1(x), decode=False)\n",
        "    x = x + attention_output\n",
        "    mlp_output = self.mlp(self.norm2(x))\n",
        "    return x + mlp_output\n",
        "\n",
        "class Transformer(nnx.Module, pytree=False):\n",
        "  def __init__(self, vocab_size: int, num_layers: int, hidden_size: int, num_heads: int = 8, *, rngs: nnx.Rngs):\n",
        "    self.embedding = nnx.Embed(vocab_size, hidden_size, rngs=rngs)\n",
        "    self.layers = [TransformerBlock(hidden_size, num_heads, rngs=rngs) for _ in range(num_layers)]\n",
        "    self.final_layer = nnx.Linear(hidden_size, vocab_size, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jax.Array):\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    logits = self.final_layer(x)\n",
        "    return logits"
      ],
      "outputs": [],
      "execution_count": 13
    },
    {
      "metadata": {
        "id": "QEcf3p7fqoJ0"
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 512\n",
        "num_heads = 8\n",
        "num_layers = 12\n",
        "vocab_size = 10000\n",
        "data_size = 8192\n",
        "sequence_length = 256\n",
        "batch_size = 32\n",
        "accumulation_steps = 16\n",
        "\n",
        "model = Transformer(vocab_size, num_layers, hidden_size, num_heads, rngs=nnx.Rngs(0))\n",
        "key = jax.random.key(0)\n",
        "batch = jax.random.randint(key, (batch_size, sequence_length), 0, vocab_size)\n",
        "\n",
        "graphdef, params = nnx.split(model, nnx.Variable)\n",
        "print(optax.tree.size(params))\n",
        "\n",
        "adamw = optax.adamw(0.01)\n",
        "opt_state = adamw.init(params)"
      ],
      "outputs": [],
      "execution_count": 14
    },
    {
      "metadata": {
        "id": "iR9qzYp6qjDS"
      },
      "cell_type": "code",
      "source": [
        "def loss_fn(params, batch):\n",
        "  model = nnx.merge(graphdef, params)\n",
        "  logits = model(batch)\n",
        "  return optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits[:, :-1], labels=batch[:,1:]\n",
        "  ).mean()\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, donate_argnums=(0,1))\n",
        "def update_fn(params, opt_state, batch):\n",
        "  loss, grads = jax.value_and_grad(loss_fn)(params, batch)\n",
        "  updates, opt_state = adamw.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state\n",
        "\n",
        "update_fn.lower(params, opt_state, batch).compile()"
      ],
      "outputs": [],
      "execution_count": 15
    },
    {
      "metadata": {
        "id": "KlRFCRc5nHLP"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Microbatching for Gradient Accumulation\n",
        "\n",
        "This section compares three methdos for performing gradient accumulation in jax/optax, one of which is through the microbatching API."
      ]
    },
    {
      "metadata": {
        "id": "COxGVTPUq9bZ"
      },
      "cell_type": "markdown",
      "source": [
        "### Option 1: Manual Gradient Accumulation\n",
        "\n",
        "64 is the largest batch size we can use with this combination of model, sequence length, and hardware.  To use larger batch sizes we have multiple options.  The first we will explore is manual, bypassing any optax abstractions.  \n",
        "\n",
        "Specifically, we will write two functions:\n",
        "1) One that computes gradients and adds them to an accumulator.\n",
        "2) One that takes accumulated gradients, performs the optimizer step, and resets the accumulated gradients back to zero."
      ]
    },
    {
      "metadata": {
        "id": "SEf4ohj-q56F"
      },
      "cell_type": "code",
      "source": [
        "@functools.partial(jax.jit, donate_argnums=(2,))\n",
        "def add_gradient(params, batch, accumulated_gradients):\n",
        "  grad = jax.grad(loss_fn)(params, batch)\n",
        "  return jax.tree.map(jnp.add, grad, accumulated_gradients)\n",
        "\n",
        "\n",
        "@functools.partial(jax.jit, donate_argnums=(0, 1, 2))\n",
        "def update_params(params, opt_state, accumulated_gradients):\n",
        "  updates, opt_state = adamw.update(accumulated_gradients, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, optax.tree.zeros_like(accumulated_gradients)\n",
        "\n",
        "accumulated_gradients = optax.tree.zeros_like(params)\n",
        "add_gradient.lower(params, batch, accumulated_gradients).compile()\n",
        "update_params.lower(params, opt_state, accumulated_gradients).compile()"
      ],
      "outputs": [],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "FjIvz_kRrAXH"
      },
      "cell_type": "code",
      "source": [
        "start_time = time.perf_counter()\n",
        "for i in range(accumulation_steps):\n",
        "  accumulated_gradients = add_gradient(params, batch, accumulated_gradients)\n",
        "\n",
        "params, opt_state, accumulated_gradients = jax.block_until_ready(\n",
        "    update_params(params, opt_state, accumulated_gradients)\n",
        ")\n",
        "end_time = time.perf_counter()\n",
        "print('Total Time', end_time - start_time)"
      ],
      "outputs": [],
      "execution_count": 9
    },
    {
      "metadata": {
        "id": "gPfNKr5RrHmq"
      },
      "cell_type": "markdown",
      "source": [
        "### Option 2: optax.MultiSteps\n",
        "\n",
        "By wrapping our optimizer with optax.MultiSteps, we can have optax handle the gradient accumulation for us.  Now we only have to define and compile a single update_fn, which is slightly simpler. The opt_state now keeps track of the accumulated gradients for us. This is more convenient as we have only a single jitted function now."
      ]
    },
    {
      "metadata": {
        "id": "vTMm84VyrDZV"
      },
      "cell_type": "code",
      "source": [
        "multi_adam = optax.MultiSteps(adamw, accumulation_steps)\n",
        "\n",
        "@functools.partial(jax.jit, donate_argnums=(0, 2))\n",
        "def update_fn_v2(params, batch, opt_state):\n",
        "  grads = jax.grad(loss_fn)(params, batch)\n",
        "  updates, opt_state = multi_adam.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state\n",
        "\n",
        "multi_opt_state = multi_adam.init(params)\n",
        "update_fn_v2.lower(params, batch, multi_opt_state).compile()"
      ],
      "outputs": [],
      "execution_count": 11
    },
    {
      "metadata": {
        "id": "qi-zg9-5rGsg"
      },
      "cell_type": "code",
      "source": [
        "start_time = time.perf_counter()\n",
        "for i in range(accumulation_steps):\n",
        "  params, multi_opt_state = update_fn_v2(params, batch, multi_opt_state)\n",
        "\n",
        "jax.block_until_ready((params, multi_opt_state))\n",
        "end_time = time.perf_counter()\n",
        "print('Total Time', end_time - start_time)"
      ],
      "outputs": [],
      "execution_count": 12
    },
    {
      "metadata": {
        "id": "SBGm3wY5rUIH"
      },
      "cell_type": "markdown",
      "source": [
        "### Option 3: `microbatching.microbatch`\n",
        "\n",
        "microbatching differs from the approach above in that it transfers the entire batch of data to device memory, then splits it up perfoming the forward-backward pass on smaller batches and accumulating them using jax.lax.scan. Like Option 2 above, the full train step can be written as a single jitted function, however now the train step is doing 16X as much work."
      ]
    },
    {
      "metadata": {
        "id": "I5HrKPuVrQiA"
      },
      "cell_type": "markdown",
      "source": []
    },
    {
      "metadata": {
        "id": "aBatfdC7rO--"
      },
      "cell_type": "code",
      "source": [
        "@functools.partial(jax.jit, donate_argnums=(0, 2))\n",
        "def update_fn_v3(params, batch, opt_state):\n",
        "  grads = microbatching.microbatch(\n",
        "      jax.grad(loss_fn),\n",
        "      argnums=1,\n",
        "      microbatch_size=batch_size,\n",
        "  )(params, batch)\n",
        "  updates, opt_state = adamw.update(grads, opt_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state\n",
        "\n",
        "full_batch = jnp.vstack([batch]*accumulation_steps)\n",
        "update_fn_v3.lower(params, full_batch, opt_state).compile()"
      ],
      "outputs": [],
      "execution_count": 14
    },
    {
      "metadata": {
        "id": "JKxDiYD9rXMF"
      },
      "cell_type": "code",
      "source": [
        "start_time = time.perf_counter()\n",
        "params, opt_state = jax.block_until_ready(update_fn_v3(params, full_batch, opt_state))\n",
        "end_time = time.perf_counter()\n",
        "print('Total Time', end_time - start_time)"
      ],
      "outputs": [],
      "execution_count": 17
    },
    {
      "metadata": {
        "id": "MZ2Xwu9znYYl"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: `microbatching.micro_vmap`\n",
        "\n",
        "micro_vmap combines microbatching with jax.vmap, providing a new transformation with a similar API as jax.vmap, but that works with much larger batches than jax.vmap. It is especially useful when the function being vmapped requries more memory than that of the inputs/outputs for intermediates, or if you want to aggregate across the vmapped dimension."
      ]
    },
    {
      "metadata": {
        "id": "fd4XmvYjnoh3"
      },
      "cell_type": "code",
      "source": [
        "def expensive_function(x):\n",
        "  return jax.nn.softmax(jnp.sin(jnp.outer(x, x))).sum(axis=0)\n",
        "\n",
        "B = 1024\n",
        "N = 4096\n",
        "X = jax.random.normal(jax.random.key(0), (B, N))\n",
        "\n",
        "# processing more examples at a time can cause ResourceExhausted errors.\n",
        "result = jax.jit(jax.vmap(expensive_function))(X[:32])\n",
        "result = jax.block_until_ready(result)\n",
        "gc.collect()\n",
        "print('Processed small batch', result.shape)\n",
        "print(result)"
      ],
      "outputs": [],
      "execution_count": 12
    },
    {
      "metadata": {
        "id": "nFGg2x5erilE"
      },
      "cell_type": "code",
      "source": [
        "result = microbatching.micro_vmap(expensive_function, microbatch_size=32)(X)\n",
        "result = jax.block_until_ready(result)\n",
        "gc.collect()\n",
        "print('Processed Full Batch', result.shape)\n",
        "print(result)"
      ],
      "outputs": [],
      "execution_count": 11
    },
    {
      "metadata": {
        "id": "PWuT3BU1xJ2S"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3: `microbatching.micro_grad`\n",
        "\n",
        "micro_grad provides a simple and performant way to compute a sum or average of transformed per-example grads. While normally computing per-example gradients with jax is more expensive than computing normal gradients, and fail to run for the same batch sizes, the microbatching provides a sound mechanism to bypass this issue that we surface through the convenient and familiar API. Below we use the API to collect metrics about the per-example gradients, which can be useful for understanding and debugging the behavior of training runs."
      ]
    },
    {
      "metadata": {
        "id": "3hPNgICJuskf"
      },
      "cell_type": "code",
      "source": [
        "def metrics_fn(per_example_grad):\n",
        "  leaf_norms = jax.tree.map(jnp.linalg.norm, per_example_grad)\n",
        "  return leaf_norms\n",
        "\n",
        "grad_fn = microbatching.micro_grad(loss_fn, metrics_fn=metrics_fn, microbatch_size=8)\n",
        "\n",
        "grad, aux = jax.jit(grad_fn)(params, batch)"
      ],
      "outputs": [],
      "execution_count": 25
    },
    {
      "metadata": {
        "id": "ph9T_WAWykls"
      },
      "cell_type": "code",
      "source": [
        "# This shows the norm for the gradient of the embedding layer per example.\n",
        "# High uniformity of the norm values is encouraging.\n",
        "aux.metrics['embedding']['embedding'].get_value()"
      ],
      "outputs": [],
      "execution_count": 29
    },
    {
      "metadata": {
        "id": "XaMThtUpnakg"
      },
      "cell_type": "markdown",
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
