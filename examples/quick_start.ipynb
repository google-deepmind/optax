{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDk-e7Pg_EdD"
      },
      "source": [
        "# Quickstart with Optax.\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/google-deepmind/optax/blob/master/examples/quick_start.ipynb)\n",
        "\n",
        "Optax is a simple optimization library for [Jax](https://jax.readthedocs.io/). The main object is the `GradientTransformation`, which can be chained\n",
        "with other transformations to obtain the final update operation and the optimizer state.\n",
        "Optax also contains some simple loss functions and utilities to help you write the full optimization steps. This notebook walks you through a few examples on how to use Optax to train a simple linear model. Begin by importing the necessary packages:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr7_e_ZJ_hky"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import optax\n",
        "import functools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7kMS9kyM8vM"
      },
      "source": [
        "In this example, we begin by setting up a simple linear model and a loss function. You can use any other library, such as [haiku](https://github.com/deepmind/dm-haiku) or [Flax](https://github.com/google/flax) to construct your networks. Here, we keep it simple and write it ourselves. The loss function (L2 Loss) comes from optax's [common loss functions](https://optax.readthedocs.io/en/latest/api.html#common-losses) via `optax.l2_loss()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-8XwoQF_AO2"
      },
      "outputs": [],
      "source": [
        "@functools.partial(jax.vmap, in_axes=(None, 0))\n",
        "def network(params, x):\n",
        "  return jnp.dot(params, x)\n",
        "\n",
        "def compute_loss(params, x, y):\n",
        "  y_pred = network(params, x)\n",
        "  loss = jnp.mean(optax.l2_loss(y_pred, y))\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZviuSmuNFsC"
      },
      "source": [
        "Here we generate data under a known linear model (with `target_params=0.5`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-_pwBx6_keL"
      },
      "outputs": [],
      "source": [
        "key = jax.random.PRNGKey(42)\n",
        "target_params = 0.5\n",
        "\n",
        "# Generate some data.\n",
        "xs = jax.random.normal(key, (16, 2))\n",
        "ys = jnp.sum(xs * target_params, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td4Lp3qDNsL3"
      },
      "source": [
        "## Basic usage of Optax\n",
        "\n",
        "Optax contains implementations of [many popular optimizers](https://optax.readthedocs.io/en/latest/api.html#Common-Optimizers) that can be used very simply. For example, the gradient transform for the Adam optimizer is available at `optax.adam()`. For now, let's start by calling the `GradientTransform` object for Adam the `optimizer`. We then initialize the optimizer state using the `init` function and `params` of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsLXLb5wBeY2"
      },
      "outputs": [],
      "source": [
        "start_learning_rate = 1e-1\n",
        "optimizer = optax.adam(start_learning_rate)\n",
        "\n",
        "# Initialize parameters of the model + optimizer.\n",
        "params = jnp.array([0.0, 0.0])\n",
        "opt_state = optimizer.init(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpAvP1WSnsyM"
      },
      "source": [
        "\n",
        "\n",
        "Next we write the update loop. The `GradientTransform` object contains an `update` function that takes in the current optimizer state and gradients and returns the `updates` that need to be applied to the parameters: `updates, new_opt_state = optimizer.update(grads, opt_state)`.\n",
        "\n",
        "Optax comes with a few simple [update rules](https://optax.readthedocs.io/en/latest/api.html#apply-updates) that apply the updates from the gradient transforms to the current parameters to return new ones: `new_params = optax.apply_updates(params, updates)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNkhz_nrB2lx"
      },
      "outputs": [],
      "source": [
        "# A simple update loop.\n",
        "for _ in range(1000):\n",
        "  grads = jax.grad(compute_loss)(params, xs, ys)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "\n",
        "assert jnp.allclose(params, target_params), \\\n",
        "'Optimization should retrive the target params used to generate the data.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXEz3j7wPZUH"
      },
      "source": [
        "### Custom optimizers\n",
        "\n",
        "Optax makes it easy to create custom optimizers by `chain`ing gradient transforms. For example, this creates an optimizer based on `adam`. Note that the scaling is `-learning_rate` which is an important detail since `apply_updates` is additive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQNI2P3YEEgP"
      },
      "outputs": [],
      "source": [
        "# Exponential decay of the learning rate.\n",
        "scheduler = optax.exponential_decay(\n",
        "    init_value=start_learning_rate, \n",
        "    transition_steps=1000,\n",
        "    decay_rate=0.99)\n",
        "\n",
        "# Combining gradient transforms using `optax.chain`.\n",
        "gradient_transform = optax.chain(\n",
        "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
        "    optax.scale_by_adam(),  # Use the updates from adam.\n",
        "    optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n",
        "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
        "    optax.scale(-1.0)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGUrLKxAEO3j"
      },
      "outputs": [],
      "source": [
        "# Initialize parameters of the model + optimizer.\n",
        "params = jnp.array([0.0, 0.0])  # Recall target_params=0.5.\n",
        "opt_state = gradient_transform.init(params)\n",
        "\n",
        "# A simple update loop.\n",
        "for _ in range(1000):\n",
        "  grads = jax.grad(compute_loss)(params, xs, ys)\n",
        "  updates, opt_state = gradient_transform.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "\n",
        "assert jnp.allclose(params, target_params), \\\n",
        "'Optimization should retrive the target params used to generate the data.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIxKL7WsXFl8"
      },
      "source": [
        "## Advanced usage of Optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCtNiVTsZVt2"
      },
      "source": [
        "### Modifying hyperparameters of optimizers in a schedule.\n",
        "\n",
        "In some scenarios, changing the hyperparameters (other than the learning rate) of an optimizer can be useful to ensure training reliability. We can do this easily by using `optax.inject_hyperparameters`. For example, this piece of code decays the `max_norm` of the `clip_by_global_norm` gradient transform as training progresses:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR9Flsj7ZdpC"
      },
      "outputs": [],
      "source": [
        "decaying_global_norm_tx = optax.inject_hyperparams(optax.clip_by_global_norm)(\n",
        "    max_norm=optax.linear_schedule(1.0, 0.0, transition_steps=99))\n",
        "\n",
        "opt_state = decaying_global_norm_tx.init(None)\n",
        "assert opt_state.hyperparams['max_norm'] == 1.0, 'Max norm should start at 1.0'\n",
        "\n",
        "for _ in range(100):\n",
        "  _, opt_state = decaying_global_norm_tx.update(None, opt_state)\n",
        "\n",
        "assert opt_state.hyperparams['max_norm'] == 0.0, 'Max norm should end at 0.0'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "private"
      },
      "name": "Quickstart with Optax",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
