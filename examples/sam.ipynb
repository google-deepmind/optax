{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEIr4kn4W6Zs"
      },
      "source": [
        "This serves a testing ground for a simple SAM type optimizer implementation in JAX with existing apis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxR7ryYMXHcr"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import optax\n",
        "import flax\n",
        "import chex\n",
        "from optax.contrib import sam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TocZvhcDJoyY"
      },
      "source": [
        "One way to describe what SAM does is that it does some number of steps (usually 1) of adversarial updates, followed by an outer gradient update.\n",
        "\n",
        "What this means is that we have to do a bunch of steps:\n",
        "\n",
        "\n",
        "    #adversarial step\n",
        "    params = params + sam_rho * normalize(gradient)\n",
        "\n",
        "    #outer update step\n",
        "    params = cache - learning_rate * gradient\n",
        "    cache = params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-p_W8vkhnO1"
      },
      "source": [
        "To actually use SAM then, you create your adversarial optimizer, here SGD with normalized gradients, and then wrap it with SAM itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueMKkNw7jLNJ"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "rho = 0.1\n",
        "adv_opt = optax.chain(sam.normalize(), optax.sgd(rho))\n",
        "opt = sam.sam(lr, adv_opt, sync_period=2)   # This is the drop-in SAM optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FesjRbUsT80"
      },
      "outputs": [],
      "source": [
        "sgd_opt = optax.sgd(lr) # baseline comparison optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyTChHZr2Yw6"
      },
      "source": [
        "We'll set up a simple test problem below, we're going to try to optimize a sum of two exponentials that has two minima, one at (0,0) and another at (2,0) and compare the performance of both SAM and ordinary SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSE3mM2FZGio"
      },
      "outputs": [],
      "source": [
        "# An example 2D loss function. It has two minima at (0,0) and (2,0).\n",
        "# Both points attain almost zero loss value, but the first one is much sharper.\n",
        "\n",
        "def loss(params):\n",
        "  x, y = params\n",
        "  return -np.exp(-(x - 2)**2 - y**2) - 1.0*np.exp(-((x)**2 + (y)**2*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi3tzM1AZbN_"
      },
      "outputs": [],
      "source": [
        "params = np.array([-0.4, -0.4])\n",
        "\n",
        "@chex.dataclass\n",
        "class Store:\n",
        "  params: chex.Array\n",
        "  state: optax.OptState\n",
        "  step: int = 0\n",
        "\n",
        "store = Store(params=params, state=opt.init(params))\n",
        "sgd_store = Store(params=params, state=sgd_opt.init(params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhFr0AwqZjRk"
      },
      "outputs": [],
      "source": [
        "def make_step(opt):\n",
        "  @jax.jit\n",
        "  def step(store):\n",
        "    value, grads = jax.value_and_grad(loss)(store.params)\n",
        "    updates, state = opt.update(grads, store.state, store.params)\n",
        "    params = optax.apply_updates(store.params, updates)\n",
        "    return store.replace(\n",
        "        params=params,\n",
        "        state=state,\n",
        "        step=store.step+1), value\n",
        "  return step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTkjju6IinJx"
      },
      "outputs": [],
      "source": [
        "step = make_step(opt)\n",
        "sgd_step = make_step(sgd_opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEF-PriWcLSa"
      },
      "outputs": [],
      "source": [
        "vals = []\n",
        "params = []\n",
        "sgd_vals = []\n",
        "sgd_params = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Em3xy9PaEbH"
      },
      "outputs": [],
      "source": [
        "T = 1000\n",
        "for i in range(T):\n",
        "  for j in range(100):\n",
        "    store, val = step(store);\n",
        "    sgd_store, sgd_val = sgd_step(sgd_store);\n",
        "  vals.append(val)\n",
        "  sgd_vals.append(sgd_val)\n",
        "  params.append(store.params)\n",
        "  sgd_params.append(sgd_store.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCrf_qJzdDmk"
      },
      "outputs": [],
      "source": [
        "ts = np.arange(T)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(ts, vals, label='SAM', lw=3)\n",
        "axs[0].plot(ts, sgd_vals, label='SGD')\n",
        "axs[0].legend();\n",
        "axs[1].plot(ts / 2, vals, label='1/2 SAM', lw=3)\n",
        "axs[1].plot(ts, sgd_vals, label='SGD')\n",
        "axs[1].legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1kIeonUeA0x"
      },
      "outputs": [],
      "source": [
        "plt.plot(*np.array(params).T, label='SAM')\n",
        "plt.plot(*np.array(sgd_params).T, label='SGD')\n",
        "plt.legend(loc=4);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nVNiUsweApJ"
      },
      "source": [
        "As you can see, the SAM optimizer finds the correct optimum, while SGD gets stuck in the local optimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_yHffHjeAjA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8zw8cVvOlmO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1ap7lwdf3vgoQdyNogmSVfncYZcH1ijch",
          "timestamp": 1697129954516
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
