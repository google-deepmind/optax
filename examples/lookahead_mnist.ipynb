{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_LlXHYcmRaC"
      },
      "source": [
        "# Lookahead Optimizer on MNIST\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/google-deepmind/optax/blob/main/examples/lookahead_mnist.ipynb)\n",
        "\n",
        "This notebook trains a simple Convolution Neural Network (CNN) for hand-written digit recognition (MNIST dataset) using {py:func}`optax.lookahead`.\n",
        "\n",
        "To run the colab locally you need install the\n",
        "`grain`, `tensorflow-datasets` packages via `pip`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cu0kFNrnJj7"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from flax import nnx\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import grain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Adl_l_uZs1d"
      },
      "outputs": [],
      "source": [
        "# @markdown The learning rate for the fast optimizer:\n",
        "FAST_LEARNING_RATE = 0.002 # @param{type:\"number\"}\n",
        "# @markdown The learning rate for the slow optimizer:\n",
        "SLOW_LEARNING_RATE = 0.1 # @param{type:\"number\"}\n",
        "# @markdown Number of fast optimizer steps to take before synchronizing parameters:\n",
        "SYNC_PERIOD = 5 # @param{type:\"integer\"}\n",
        "# @markdown Number of samples in each batch:\n",
        "BATCH_SIZE = 256 # @param{type:\"integer\"}\n",
        "# @markdown Total number of epochs to train for:\n",
        "N_EPOCHS = 1 # @param{type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZej3FcOhuRE"
      },
      "source": [
        "MNIST is a dataset of 28x28 images with 1 channel. We now load the dataset using `tensorflow_datasets`, convert to grain dataset using `grain.MapDataset` and apply min-max normalization to images, shuffle the data in the train set and create batches of size `BATCH_SIZE`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPZ0paOehHWg"
      },
      "outputs": [],
      "source": [
        "train_source, test_source = tfds.data_source(\"mnist\", split=[\"train\", \"test\"])\n",
        "\n",
        "IMG_SIZE = train_source.dataset_info.features[\"image\"].shape\n",
        "NUM_CLASSES = train_source.dataset_info.features[\"label\"].num_classes\n",
        "\n",
        "train_loader_batched = (\n",
        "    grain.MapDataset.source(train_source)\n",
        "    .shuffle(seed=45)\n",
        "    .map(lambda x: (x[\"image\"] / 255., x[\"label\"]))\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        ")\n",
        "\n",
        "test_loader_batched = (\n",
        "    grain.MapDataset.source(test_source)\n",
        "    .map(lambda x: (x[\"image\"] / 255., x[\"label\"]))\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkLaC2MlbAqa"
      },
      "source": [
        "The data is ready! Next let's define a model. Optax is agnostic to which (if any) neural network library is used. Here we use Flax to implement a simple CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RppusWrcaXzX"
      },
      "outputs": [],
      "source": [
        "class CNN(nnx.Module):\n",
        "  \"\"\"A simple CNN model.\"\"\"\n",
        "\n",
        "  def __init__(self, in_features, rngs: nnx.Rngs):\n",
        "    self.conv1 = nnx.Conv(in_features=in_features,\n",
        "                          out_features=32,\n",
        "                          kernel_size=(3, 3),\n",
        "                          rngs=rngs)\n",
        "    self.pool1 = partial(nnx.avg_pool, window_shape=(2, 2), strides=(2, 2))\n",
        "    self.conv2 = nnx.Conv(in_features=32,\n",
        "                          out_features=64,\n",
        "                          kernel_size=(3, 3),\n",
        "                          rngs=rngs)\n",
        "    self.pool2 = partial(nnx.avg_pool, window_shape=(2, 2), strides=(2, 2))\n",
        "    self.dense1 = nnx.Linear(in_features=7*7*64, out_features=256, rngs=rngs)\n",
        "    self.dense2 = nnx.Linear(in_features=256, out_features=10, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = nnx.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = nnx.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = x.reshape((x.shape[0], -1))  # flatten\n",
        "    x = self.dense1(x)\n",
        "    x = nnx.relu(x)\n",
        "    x = self.dense2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKOi55MgdPyp"
      },
      "outputs": [],
      "source": [
        "net = CNN(IMG_SIZE[-1], nnx.Rngs(0))\n",
        "\n",
        "@jax.jit\n",
        "def predict(params, model, inputs):\n",
        "  nnx.update(model, params)\n",
        "  return model(inputs)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def loss_accuracy(params, model, data):\n",
        "  \"\"\"Computes loss and accuracy over a mini-batch.\n",
        "\n",
        "  Args:\n",
        "    params: parameters of the model.\n",
        "    model: Instance of the nnx.Module.\n",
        "    data: tuple of (inputs, labels).\n",
        "\n",
        "  Returns:\n",
        "    loss: float\n",
        "    accuracy: float\n",
        "  \"\"\"\n",
        "  inputs, labels = data\n",
        "  logits = predict(params, model, inputs)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=labels\n",
        "  ).mean()\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n",
        "  return loss, {\"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eB2dhIpjTIi"
      },
      "source": [
        "Next we need to initialize CNN parameters and solver state. We also define a convenience function `dataset_stats` that we'll call once per epoch to collect the loss and accuracy of our solver over the test set. We will be using the Lookahead optimizer.\n",
        "Its wrapper keeps a pair of slow and fast parameters. To\n",
        "initialize them, we create a pair of synchronized parameters from the\n",
        "initial model parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBnbq7gui34L"
      },
      "outputs": [],
      "source": [
        "fast_solver = optax.adam(FAST_LEARNING_RATE)\n",
        "solver = optax.lookahead(fast_solver, SYNC_PERIOD, SLOW_LEARNING_RATE)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "dummy_data = jnp.ones((1,) + IMG_SIZE, dtype=jnp.float32)\n",
        "\n",
        "_ = net(dummy_data)\n",
        "params = nnx.state(net, nnx.Param)\n",
        "\n",
        "# Initializes the lookahead optimizer with the initial model parameters.\n",
        "params = optax.LookaheadParams.init_synced(params)\n",
        "solver_state = solver.init(params)\n",
        "\n",
        "def dataset_stats(params, model, data_loader):\n",
        "  \"\"\"Computes loss and accuracy over the dataset `data_loader`.\"\"\"\n",
        "  all_accuracy = []\n",
        "  all_loss = []\n",
        "  for batch in data_loader:\n",
        "    batch_loss, batch_aux = loss_accuracy(params, model, batch)\n",
        "    all_loss.append(batch_loss)\n",
        "    all_accuracy.append(batch_aux[\"accuracy\"])\n",
        "  return {\"loss\": jnp.mean(jnp.array(all_loss)),\n",
        "          \"accuracy\": jnp.mean(jnp.array(all_accuracy))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H6GWNJf0XTY"
      },
      "source": [
        "Finally, we do the actual training. The next cell train the model for  `N_EPOCHS`. Within each epoch we iterate over the batched loader `train_loader_batched`, and once per epoch we also compute the test set accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeQr0urBjoDj"
      },
      "outputs": [],
      "source": [
        "train_accuracy = []\n",
        "train_losses = []\n",
        "\n",
        "# Computes test set accuracy at initialization.\n",
        "test_stats = dataset_stats(params.slow, net, test_loader_batched)\n",
        "test_accuracy = [test_stats[\"accuracy\"]]\n",
        "test_losses = [test_stats[\"loss\"]]\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, model, solver_state, batch):\n",
        "  # Performs a one step update.\n",
        "  (loss, aux), grad = jax.value_and_grad(loss_accuracy, has_aux=True)(\n",
        "      params.fast, model, batch\n",
        "  )\n",
        "  updates, solver_state = solver.update(grad, solver_state, params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, solver_state, loss, aux\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  train_accuracy_epoch = []\n",
        "  train_losses_epoch = []\n",
        "\n",
        "  for step, train_batch in enumerate(train_loader_batched):\n",
        "    params, solver_state, train_loss, train_aux = train_step(\n",
        "        params, net, solver_state, train_batch\n",
        "    )\n",
        "    train_accuracy_epoch.append(train_aux[\"accuracy\"])\n",
        "    train_losses_epoch.append(train_loss)\n",
        "    if step % 20 == 0:\n",
        "      print(\n",
        "          f\"step {step}, train loss: {train_loss:.2e}, train accuracy:\"\n",
        "          f\" {train_aux['accuracy']:.2f}\"\n",
        "      )\n",
        "\n",
        "  # Validation is done on the slow lookahead parameters.\n",
        "  test_stats = dataset_stats(params.slow, net, test_loader_batched)\n",
        "  test_accuracy.append(test_stats[\"accuracy\"])\n",
        "  test_losses.append(test_stats[\"loss\"])\n",
        "  train_accuracy.append(jnp.mean(jnp.array(train_accuracy_epoch)))\n",
        "  train_losses.append(jnp.mean(jnp.array(train_losses_epoch)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyS1oRZBtytP"
      },
      "outputs": [],
      "source": [
        "f\"Improved accuracy on test DS from {test_accuracy[0]} to {test_accuracy[-1]}\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "execution": {
      "timeout": -1
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
